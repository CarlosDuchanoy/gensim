{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of FastText and Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Facebook Research open sourced a great project recently - [fastText](https://github.com/facebookresearch/fastText), a fast (no surprise) and effective method to learn word representations and perform text classification. I was curious about comparing these embeddings to other commonly used embeddings, so word2vec seemed like the obvious choice, especially considering fastText embeddings are an extension of word2vec. \n",
    "\n",
    "I've used gensim to train the word2vec models, and the analogical reasoning task (described in Section 4.1 of [[2]](https://arxiv.org/pdf/1301.3781v3.pdf)) for comparing the word2vec and fastText models. I've compared embeddings trained using the skipgram architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n",
      "--2016-08-11 01:55:34--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net (mattmahoney.net)... 98.139.135.129\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|98.139.135.129|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘text8.zip’\n",
      "\n",
      "100%[======================================>] 3,13,44,016  446KB/s   in 1m 52s \n",
      "\n",
      "2016-08-11 01:57:26 (274 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
      "\n",
      "Archive:  text8.zip\n",
      "  inflating: text8                   \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown') \n",
    "# Only the brown corpus is needed in case you don't have it.\n",
    "\n",
    "# Generate brown corpus text file\n",
    "with open('brown_corp.txt', 'w+') as f:\n",
    "    for word in nltk.corpus.brown.words():\n",
    "        f.write('{word} '.format(word=word))\n",
    "\n",
    "# download the text8 corpus (a 100 MB sample of cleaned wikipedia text)\n",
    "import os.path\n",
    "if not os.path.isfile('text8'):\n",
    "    !wget http://mattmahoney.net/dc/text8.zip\n",
    "    !unzip text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training the models yourself, you'll need to have both [Gensim](https://github.com/RaRe-Technologies/gensim) and [FastText](https://github.com/facebookresearch/fastText) set up on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fasttext on brown corpus..\n",
      "Read 1M words\n",
      "Progress: 100.0%  words/sec/thread: 40798  lr: 0.000001  loss: 2.291000  eta: 0h0m \n",
      "Train time: 39.000000 sec\n",
      "CPU times: user 1.09 s, sys: 104 ms, total: 1.19 s\n",
      "Wall time: 44.8 s\n",
      "\n",
      "Training fasttext on brown corpus (without char n-grams)..\n",
      "Read 1M words\n",
      "Progress: 100.0%  words/sec/thread: 74234  lr: 0.000001  loss: 2.360271  eta: 0h0m \n",
      "Train time: 22.000000 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 756 ms, sys: 88 ms, total: 844 ms\n",
      "Wall time: 27.4 s\n",
      "\n",
      "Training word2vec on brown corpus..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-21 11:40:29,440 : INFO : collecting all words and their counts\n",
      "2016-08-21 11:40:29,444 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2016-08-21 11:40:29,838 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 117 sentences\n",
      "2016-08-21 11:40:30,012 : INFO : min_count=5 retains 15173 unique words (drops 40884)\n",
      "2016-08-21 11:40:30,013 : INFO : min_count leaves 1095086 word corpus (94% of original 1161192)\n",
      "2016-08-21 11:40:30,063 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2016-08-21 11:40:30,067 : INFO : sample=0.0001 downsamples 340 most-common words\n",
      "2016-08-21 11:40:30,068 : INFO : downsampling leaves estimated 540252 word corpus (49.3% of prior 1095086)\n",
      "2016-08-21 11:40:30,068 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
      "2016-08-21 11:40:30,134 : INFO : resetting layer weights\n",
      "2016-08-21 11:40:30,304 : INFO : training model with 3 workers on 15173 vocabulary and 100 features, using sg=1 hs=0 sample=0.0001 negative=5\n",
      "2016-08-21 11:40:30,305 : INFO : expecting 117 sentences, matching count from corpus used for vocabulary survey\n",
      "2016-08-21 11:40:31,326 : INFO : PROGRESS: at 9.40% examples, 252737 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:32,356 : INFO : PROGRESS: at 19.32% examples, 256979 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:33,367 : INFO : PROGRESS: at 29.57% examples, 262210 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:34,386 : INFO : PROGRESS: at 40.85% examples, 270704 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:35,434 : INFO : PROGRESS: at 47.69% examples, 252080 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:36,457 : INFO : PROGRESS: at 49.06% examples, 216017 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:37,462 : INFO : PROGRESS: at 54.87% examples, 208538 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:38,472 : INFO : PROGRESS: at 66.15% examples, 219387 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:39,477 : INFO : PROGRESS: at 76.75% examples, 226969 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:40,505 : INFO : PROGRESS: at 88.03% examples, 233569 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:41,598 : INFO : PROGRESS: at 93.85% examples, 225348 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:42,881 : INFO : PROGRESS: at 95.21% examples, 205307 words/s, in_qsize 5, out_qsize 0\n",
      "2016-08-21 11:40:43,325 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2016-08-21 11:40:43,335 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2016-08-21 11:40:43,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2016-08-21 11:40:43,351 : INFO : training on 5805960 raw words (2701067 effective words) took 13.0s, 207071 effective words/s\n",
      "2016-08-21 11:40:43,360 : INFO : storing 15173x100 projection weights into models/brown_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36.9 s, sys: 44 ms, total: 37 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "# Make sure you set FT_HOME to your fastText directory root\n",
    "FT_HOME = 'fastText/'\n",
    "MODELS_DIR = 'models/'\n",
    "!mkdir -p {MODELS_DIR}\n",
    "\n",
    "lr = 0.05\n",
    "dim = 100\n",
    "ws = 5\n",
    "epoch = 5\n",
    "minCount = 5\n",
    "neg = 5\n",
    "loss = 'ns'\n",
    "t = 1e-4\n",
    "\n",
    "if not os.path.isfile(os.path.join(MODELS_DIR, 'brown_ft.vec')):\n",
    "    print('Training fasttext on brown corpus..')\n",
    "    %time !{FT_HOME}fasttext skipgram -input brown_corp.txt -output {MODELS_DIR+'brown_ft'}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n",
    "else:\n",
    "    print('\\nUsing existing model file brown_ft.vec')\n",
    "    \n",
    "if not os.path.isfile(os.path.join(MODELS_DIR, 'brown_ft_no_ng.vec')):\n",
    "    print('\\nTraining fasttext on brown corpus (without char n-grams)..')\n",
    "    %time !{FT_HOME}fasttext skipgram -input brown_corp.txt -output {MODELS_DIR+'brown_ft_no_ng'}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t} -maxn 0\n",
    "else:\n",
    "    print('\\nUsing existing model file brown_ft_no_ng.vec')\n",
    "    \n",
    "# Training gensim skipgram model on brown corpus\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "# Same values as used for fastText training above\n",
    "params = {\n",
    "    'alpha': lr,\n",
    "    'size': dim,\n",
    "    'window': ws,\n",
    "    'iter': epoch,\n",
    "    'min_count': minCount,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 0,\n",
    "    'negative': neg\n",
    "}\n",
    "\n",
    "if not os.path.isfile(os.path.join(MODELS_DIR, 'brown_gs.vec')):\n",
    "    print('\\nTraining word2vec on brown corpus..')\n",
    "    %time brown_gs = Word2Vec(Text8Corpus('brown_corp.txt'), **params) #Text8Corpus class for reading space-separated words file\n",
    "    brown_gs.save_word2vec_format(MODELS_DIR + 'brown_gs.vec')\n",
    "else:\n",
    "    print('\\nUsing existing model file brown_gs.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fastText skipgram model on text8 corpus...\n",
      "Read 17M words\n",
      "Progress: 100.0%  words/sec/thread: 48719  lr: 0.000001  loss: 1.835444  eta: 0h0m \n",
      "Train time: 518.000000 sec\n",
      "CPU times: user 8.52 s, sys: 944 ms, total: 9.46 s\n",
      "Wall time: 8min 51s\n",
      "\n",
      "Training fastText skipgram model on text8 corpus (without n-grams)...\n",
      "Read 17M words\n",
      "Progress: 100.0%  words/sec/thread: 107885  lr: 0.000001  loss: 1.878955  eta: 0h0m \n",
      "Train time: 219.000000 sec\n",
      "CPU times: user 3.72 s, sys: 540 ms, total: 4.26 s\n",
      "Wall time: 3min 51s\n",
      "\n",
      "Training word2vec on text8 corpus..\n",
      "CPU times: user 9min 2s, sys: 1.22 s, total: 9min 3s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(os.path.join(MODELS_DIR, 'text8_ft.vec')):\n",
    "    print(\"Training fastText skipgram model on text8 corpus...\")\n",
    "    %time !{FT_HOME}fasttext skipgram -input text8 -output {MODELS_DIR+'text8_ft'}  -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t}\n",
    "else:\n",
    "    print('\\nUsing existing model file text8_ft.vec')\n",
    "    \n",
    "if not os.path.isfile(os.path.join(MODELS_DIR, 'text8_ft_no_ng.vec')):\n",
    "    print(\"\\nTraining fastText skipgram model on text8 corpus (without n-grams)...\")\n",
    "    %time !{FT_HOME}fasttext skipgram -input text8 -output {MODELS_DIR+'text8_ft_no_ng'} -lr {lr} -dim {dim} -ws {ws} -epoch {epoch} -minCount {minCount} -neg {neg} -loss {loss} -t {t} -maxn 0\n",
    "else:\n",
    "    print('\\nUsing existing model file text8_ft_no_ng.vec')\n",
    "    \n",
    "if not os.path.isfile(os.path.join(MODELS_DIR, 'text8_gs.vec')):\n",
    "    print(\"\\nTraining word2vec on text8 corpus..\")\n",
    "    %time text8_gs = Word2Vec(Text8Corpus('text8'), **params)\n",
    "    text8_gs.save_word2vec_format(MODELS_DIR + 'text8_gs.vec')\n",
    "else:\n",
    "    print('\\nUsing existing model file text8_gs.vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-08-21 10:31:16--  https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.100.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.100.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 603955 (590K) [text/plain]\n",
      "Saving to: ‘questions-words.txt’\n",
      "\n",
      "100%[======================================>] 6,03,955     554KB/s   in 1.1s   \n",
      "\n",
      "2016-08-21 10:31:17 (554 KB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the file questions-words.txt to be used for comparing word embeddings\n",
    "!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have downloaded or trained the models and downloaded `questions-words.txt`, you're ready to run the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:00,341 : INFO : loading projection weights from models/brown_ft.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading FastText embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:01,471 : INFO : loaded (15173, 100) matrix from models/brown_ft.vec\n",
      "2016-08-11 02:14:01,525 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FastText (with n-grams):\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:01,912 : INFO : family: 16.5% (30/182)\n",
      "2016-08-11 02:14:03,225 : INFO : gram1-adjective-to-adverb: 74.4% (522/702)\n",
      "2016-08-11 02:14:03,491 : INFO : gram2-opposite: 80.3% (106/132)\n",
      "2016-08-11 02:14:05,584 : INFO : gram3-comparative: 61.4% (648/1056)\n",
      "2016-08-11 02:14:06,029 : INFO : gram4-superlative: 67.1% (141/210)\n",
      "2016-08-11 02:14:07,406 : INFO : gram5-present-participle: 65.5% (426/650)\n",
      "2016-08-11 02:14:10,038 : INFO : gram7-past-tense: 13.3% (168/1260)\n",
      "2016-08-11 02:14:11,128 : INFO : gram8-plural: 59.1% (326/552)\n",
      "2016-08-11 02:14:11,799 : INFO : gram9-plural-verbs: 72.5% (248/342)\n",
      "2016-08-11 02:14:11,801 : INFO : total: 51.4% (2615/5086)\n",
      "2016-08-11 02:14:11,803 : INFO : loading projection weights from models/brown_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 30/182, Accuracy: 16.48%\n",
      "Syntactic: 2585/4904, Accuracy: 52.71%\n",
      "\n",
      "\n",
      "Loading Gensim embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:13,067 : INFO : loaded (15173, 100) matrix from models/brown_gs.vec\n",
      "2016-08-11 02:14:13,144 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Word2Vec:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:13,517 : INFO : family: 25.3% (46/182)\n",
      "2016-08-11 02:14:14,826 : INFO : gram1-adjective-to-adverb: 0.4% (3/702)\n",
      "2016-08-11 02:14:15,093 : INFO : gram2-opposite: 0.0% (0/132)\n",
      "2016-08-11 02:14:18,479 : INFO : gram3-comparative: 3.3% (35/1056)\n",
      "2016-08-11 02:14:19,155 : INFO : gram4-superlative: 1.4% (3/210)\n",
      "2016-08-11 02:14:20,671 : INFO : gram5-present-participle: 0.8% (5/650)\n",
      "2016-08-11 02:14:23,697 : INFO : gram7-past-tense: 1.3% (16/1260)\n",
      "2016-08-11 02:14:24,753 : INFO : gram8-plural: 5.1% (28/552)\n",
      "2016-08-11 02:14:25,435 : INFO : gram9-plural-verbs: 2.3% (8/342)\n",
      "2016-08-11 02:14:25,437 : INFO : total: 2.8% (144/5086)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 46/182, Accuracy: 25.27%\n",
      "Syntactic: 98/4904, Accuracy: 2.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "def print_accuracy(model, questions_file):\n",
    "    print('Evaluating...\\n')\n",
    "    acc = model.accuracy(questions_file)\n",
    "\n",
    "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
    "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
    "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, 100*float(sem_correct)/sem_total))\n",
    "    \n",
    "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
    "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
    "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, 100*float(syn_correct)/syn_total))\n",
    "\n",
    "word_analogies_file = 'questions-words.txt'\n",
    "print('\\nLoading FastText embeddings')\n",
    "brown_ft = Word2Vec.load_word2vec_format(MODELS_DIR + 'brown_ft.vec')\n",
    "print('Accuracy for FastText (with n-grams):')\n",
    "print_accuracy(brown_ft, word_analogies_file)\n",
    "\n",
    "print('\\nLoading Gensim embeddings')\n",
    "brown_gs = Word2Vec.load_word2vec_format(MODELS_DIR + 'brown_gs.vec')\n",
    "print('Accuracy for Word2Vec:')\n",
    "print_accuracy(brown_gs, word_analogies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec embeddings seem to be slightly better than fastText embeddings at the semantic tasks, while the fastText embeddings do significantly better on the syntactic analogies. Makes sense, since fastText embeddings are trained for understanding morphological nuances, and most of the syntactic analogies are morphology based. \n",
    "\n",
    "Let me explain that better.\n",
    "\n",
    "According to the paper [[1]](https://arxiv.org/abs/1607.04606), embeddings for words are represented by the sum of their n-gram embeddings. This is meant to be useful for morphologically rich languages - so theoretically, the embedding for `apparently` would include information from both character n-grams `apparent` and `ly` (as well as other n-grams), and the n-grams would combine in a simple, linear manner. This is very similar to what most of our syntactic tasks look like.\n",
    "\n",
    "Example analogy:\n",
    "\n",
    "`amazing amazingly calm calmly`\n",
    "\n",
    "This analogy is marked correct if: \n",
    "\n",
    "`embedding(amazing)` - `embedding(amazingly)` = `embedding(calm)` - `embedding(calmly)`\n",
    "\n",
    "Both these subtractions would result in a very similar set of remaining ngrams.\n",
    "No surprise the fastText embeddings do extremely well on this.\n",
    "\n",
    "Let's do a small test to validate this hypothesis - fastText differs from word2vec only in that it uses char n-gram embeddings as well as the actual word embedding in the scoring function to calculate scores and then likelihoods for each word, given a context word. In case char n-gram embeddings are not present, this reduces (atleast theoretically) to the original word2vec model. This can be implemented by setting 0 for the max length of char n-grams for fastText.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:25,450 : INFO : loading projection weights from models/brown_ft_no_ng.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:26,645 : INFO : loaded (15173, 100) matrix from models/brown_ft_no_ng.vec\n",
      "2016-08-11 02:14:26,725 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FastText (without n-grams):\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:27,091 : INFO : family: 18.1% (33/182)\n",
      "2016-08-11 02:14:28,402 : INFO : gram1-adjective-to-adverb: 0.0% (0/702)\n",
      "2016-08-11 02:14:28,689 : INFO : gram2-opposite: 0.8% (1/132)\n",
      "2016-08-11 02:14:30,772 : INFO : gram3-comparative: 2.7% (28/1056)\n",
      "2016-08-11 02:14:31,189 : INFO : gram4-superlative: 1.0% (2/210)\n",
      "2016-08-11 02:14:32,406 : INFO : gram5-present-participle: 0.6% (4/650)\n",
      "2016-08-11 02:14:34,765 : INFO : gram7-past-tense: 1.0% (12/1260)\n",
      "2016-08-11 02:14:35,957 : INFO : gram8-plural: 6.3% (35/552)\n",
      "2016-08-11 02:14:37,044 : INFO : gram9-plural-verbs: 1.2% (4/342)\n",
      "2016-08-11 02:14:37,046 : INFO : total: 2.3% (119/5086)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 33/182, Accuracy: 18.13%\n",
      "Syntactic: 86/4904, Accuracy: 1.75%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading FastText embeddings')\n",
    "brown_ft_no_ng = Word2Vec.load_word2vec_format(MODELS_DIR + 'brown_ft_no_ng.vec')\n",
    "print('Accuracy for FastText (without n-grams):')\n",
    "print_accuracy(brown_ft_no_ng, word_analogies_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A-ha! The results for FastText with no n-grams and Word2Vec look a lot more similar (as they should) - the differences could easily result from differences in implementation between fastText and Gensim, and randomization. Especially telling is that the semantic accuracy for FastText has improved slightly after removing n-grams, while the syntactic accuracy has taken a giant dive. Our hypothesis that the char n-grams result in better performance on syntactic analogies seems fair. It also seems possible that char n-grams hurt semantic accuracy a little. However, the brown corpus is too small to be able to draw any definite conclusions - the accuracies seem vary significantly over different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try with a larger corpus now - text8 (collection of wiki articles). I'm also curious about the impact on semantic accuracy - for models trained on the brown corpus, the difference in the semantic accuracy and the accuracy values themselves are too small to be conclusive. Hopefully a larger corpus helps, and the text8 corpus likely has a lot more information about capitals, currencies, cities etc, which should be relevant to the semantic tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:37,067 : INFO : loading projection weights from models/text8_ft_no_ng.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FastText embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:43,506 : INFO : loaded (71290, 100) matrix from models/text8_ft_no_ng.vec\n",
      "2016-08-11 02:14:43,691 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FastText (without n-grams):\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:14:45,858 : INFO : capital-common-countries: 71.1% (360/506)\n",
      "2016-08-11 02:14:52,498 : INFO : capital-world: 48.6% (706/1452)\n",
      "2016-08-11 02:14:53,594 : INFO : currency: 22.0% (59/268)\n",
      "2016-08-11 02:15:00,493 : INFO : city-in-state: 22.0% (332/1511)\n",
      "2016-08-11 02:15:02,293 : INFO : family: 57.2% (175/306)\n",
      "2016-08-11 02:15:06,980 : INFO : gram1-adjective-to-adverb: 13.9% (105/756)\n",
      "2016-08-11 02:15:08,790 : INFO : gram2-opposite: 15.0% (46/306)\n",
      "2016-08-11 02:15:14,361 : INFO : gram3-comparative: 44.0% (555/1260)\n",
      "2016-08-11 02:15:16,740 : INFO : gram4-superlative: 22.3% (113/506)\n",
      "2016-08-11 02:15:20,652 : INFO : gram5-present-participle: 22.6% (224/992)\n",
      "2016-08-11 02:15:26,655 : INFO : gram6-nationality-adjective: 79.3% (1087/1371)\n",
      "2016-08-11 02:15:31,947 : INFO : gram7-past-tense: 32.7% (436/1332)\n",
      "2016-08-11 02:15:36,068 : INFO : gram8-plural: 53.2% (528/992)\n",
      "2016-08-11 02:15:39,583 : INFO : gram9-plural-verbs: 26.8% (174/650)\n",
      "2016-08-11 02:15:39,585 : INFO : total: 40.1% (4900/12208)\n",
      "2016-08-11 02:15:39,592 : INFO : loading projection weights from models/text8_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 1632/4043, Accuracy: 40.37%\n",
      "Syntactic: 3268/8165, Accuracy: 40.02%\n",
      "\n",
      "Loading Gensim embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:15:45,542 : INFO : loaded (71290, 100) matrix from models/text8_gs.vec\n",
      "2016-08-11 02:15:45,753 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for word2vec:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:15:47,957 : INFO : capital-common-countries: 68.0% (344/506)\n",
      "2016-08-11 02:15:54,026 : INFO : capital-world: 47.9% (695/1452)\n",
      "2016-08-11 02:15:55,180 : INFO : currency: 20.9% (56/268)\n",
      "2016-08-11 02:16:03,023 : INFO : city-in-state: 23.2% (365/1571)\n",
      "2016-08-11 02:16:05,472 : INFO : family: 54.2% (166/306)\n",
      "2016-08-11 02:16:09,815 : INFO : gram1-adjective-to-adverb: 16.0% (121/756)\n",
      "2016-08-11 02:16:11,688 : INFO : gram2-opposite: 16.0% (49/306)\n",
      "2016-08-11 02:16:18,558 : INFO : gram3-comparative: 55.2% (695/1260)\n",
      "2016-08-11 02:16:20,817 : INFO : gram4-superlative: 31.6% (160/506)\n",
      "2016-08-11 02:16:25,408 : INFO : gram5-present-participle: 28.0% (278/992)\n",
      "2016-08-11 02:16:31,638 : INFO : gram6-nationality-adjective: 77.2% (1059/1371)\n",
      "2016-08-11 02:16:38,305 : INFO : gram7-past-tense: 31.2% (416/1332)\n",
      "2016-08-11 02:16:42,955 : INFO : gram8-plural: 48.4% (480/992)\n",
      "2016-08-11 02:16:45,772 : INFO : gram9-plural-verbs: 30.9% (201/650)\n",
      "2016-08-11 02:16:45,774 : INFO : total: 41.4% (5085/12268)\n",
      "2016-08-11 02:16:45,781 : INFO : loading projection weights from models/text8_ft.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 1626/4103, Accuracy: 39.63%\n",
      "Syntactic: 3459/8165, Accuracy: 42.36%\n",
      "\n",
      "Loading FastText embeddings (with n-grams)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:16:51,881 : INFO : loaded (71290, 100) matrix from models/text8_ft.vec\n",
      "2016-08-11 02:16:52,076 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for FastText (with n-grams):\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-08-11 02:16:54,100 : INFO : capital-common-countries: 62.6% (317/506)\n",
      "2016-08-11 02:17:00,728 : INFO : capital-world: 43.0% (624/1452)\n",
      "2016-08-11 02:17:01,834 : INFO : currency: 11.9% (32/268)\n",
      "2016-08-11 02:17:08,550 : INFO : city-in-state: 19.5% (294/1511)\n",
      "2016-08-11 02:17:09,774 : INFO : family: 47.4% (145/306)\n",
      "2016-08-11 02:17:13,485 : INFO : gram1-adjective-to-adverb: 77.5% (586/756)\n",
      "2016-08-11 02:17:14,836 : INFO : gram2-opposite: 61.1% (187/306)\n",
      "2016-08-11 02:17:20,270 : INFO : gram3-comparative: 63.1% (795/1260)\n",
      "2016-08-11 02:17:22,524 : INFO : gram4-superlative: 59.1% (299/506)\n",
      "2016-08-11 02:17:26,654 : INFO : gram5-present-participle: 55.7% (553/992)\n",
      "2016-08-11 02:17:32,705 : INFO : gram6-nationality-adjective: 93.9% (1288/1371)\n",
      "2016-08-11 02:17:38,990 : INFO : gram7-past-tense: 36.0% (480/1332)\n",
      "2016-08-11 02:17:43,387 : INFO : gram8-plural: 88.6% (879/992)\n",
      "2016-08-11 02:17:46,392 : INFO : gram9-plural-verbs: 59.7% (388/650)\n",
      "2016-08-11 02:17:46,393 : INFO : total: 56.2% (6867/12208)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 1412/4043, Accuracy: 34.92%\n",
      "Syntactic: 5455/8165, Accuracy: 66.81%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading FastText embeddings')\n",
    "text8_ft_no_ng = Word2Vec.load_word2vec_format(MODELS_DIR + 'text8_ft_no_ng.vec')\n",
    "print('Accuracy for FastText (without n-grams):')\n",
    "print_accuracy(text8_ft_no_ng, word_analogies_file)\n",
    "\n",
    "print('Loading Gensim embeddings')\n",
    "text8_gs = Word2Vec.load_word2vec_format(MODELS_DIR + 'text8_gs.vec')\n",
    "print('Accuracy for word2vec:')\n",
    "print_accuracy(text8_gs, word_analogies_file)\n",
    "\n",
    "print('Loading FastText embeddings (with n-grams)')\n",
    "text8_ft = Word2Vec.load_word2vec_format(MODELS_DIR + 'text8_ft.vec')\n",
    "print('Accuracy for FastText (with n-grams):')\n",
    "print_accuracy(text8_ft, word_analogies_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the text8 corpus, we observe a similar pattern. Semantic accuracy falls by a small but significant amount when n-grams are included in FastText, while FastText with n-grams performs far better on the syntactic analogies. FastText without n-grams are largely similar to Word2Vec.\n",
    "\n",
    "My hypothesis for semantic accuracy being lower for the FastText-with-ngrams model is that most of the words in the semantic analogies are standalone words and are unrelated to their morphemes (eg: father, mother, France, Paris), hence inclusion of the char n-grams into the scoring function actually makes the embeddings worse.\n",
    "\n",
    "This trend is observed in the original paper too where the performance of embeddings with n-grams is worse on semantic tasks than both word2vec cbow and skipgram models.\n",
    "\n",
    "A couple of other notes - \n",
    "\n",
    "1. The semantic accuracy for all models increases significantly with the increase in corpus size.\n",
    "2. However, the increase in syntactic accuracy from the increase in corpus size for the n-gram FastText model is lower (in both relative and absolute terms) for the n-gram FastText model. This could possibly indicate that advantages gained by incorporating morphological information could be less significant in case of larger corpus sizes (the corpuses used in the original paper seem to indicate this too)\n",
    "3. Training times for gensim are slightly lower than the fastText no-ngram model, and significantly lower than the n-gram variant. This is quite impressive considering fastText is implemented in C++ and Gensim in Python. You could read [this post](http://rare-technologies.com/word2vec-in-python-part-two-optimizing/) for more details regarding word2vec optimisation in Gensim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These preliminary results seem to indicate fastText embeddings are significantly better than word2vec at encoding syntactic information. This is expected, since most syntactic analogies are morphology based, and the char n-gram approach of fastText takes such information into account. The original word2vec model seems to perform better on semantic tasks, since words in semantic analogies are unrelated to their char n-grams, and the added information from irrelevant char n-grams worsens the embeddings. It'd be interesting to see how transferable these embeddings are for different kinds of tasks by comparing their performance in a downstream supervised task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606v1.pdf)\n",
    "\n",
    "[2] [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
